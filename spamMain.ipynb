{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this encoding methods 'utf-8', 'latin1', 'ISO-8859-1', 'cp1252', 'utf-16' .\n",
    "\n",
    "utf-8 and utf-16 unable to encoding the csv file .\n",
    "{The error message specifically mentions that it can't decode bytes in position 606-607 as an invalid continuation byte in the UTF-8 encoding.}.\n",
    "\n",
    "So consider other encoding methods perform well in encoding csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('spam.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rename(columns={'v1':'Target','v2':'mailText'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target           2\n",
       "mailText      5169\n",
       "Unnamed: 2      43\n",
       "Unnamed: 3      10\n",
       "Unnamed: 4       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['mailText'] = dataset['mailText'].fillna('') + dataset['Unnamed: 2'].fillna('') + dataset['Unnamed: 3'].fillna('') + dataset['Unnamed: 4'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target           0\n",
       "mailText         0\n",
       "Unnamed: 2    5522\n",
       "Unnamed: 3    5560\n",
       "Unnamed: 4    5566\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'spam': 747\n",
      "Count of 'not spam': 4825\n"
     ]
    }
   ],
   "source": [
    "count_spam = dataset['Target'].value_counts().get('spam', 0)\n",
    "count_not_spam = dataset['Target'].value_counts().get('ham', 0)\n",
    "\n",
    "print(\"Count of 'spam':\", count_spam)\n",
    "print(\"Count of 'not spam':\", count_not_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the Raw Data using NTk(Natural Language Toolkit)\n",
    "--\n",
    "NLTK(Natural Language Toolkit) is the most popular and widely used Python library for doing Natural Language Processing(NLP) or Text Mining.\n",
    "\n",
    "NLP is one of the important parts of Artificial Intelligence(AI) that focuses on teaching computers how to extract meaning from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert text into lowercase\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText\n",
       "0    ham  go until jurong point, crazy.. available only ...\n",
       "1    ham                      ok lar... joking wif u oni...\n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3    ham  u dun say so early hor... u c already then say...\n",
       "4    ham  nah i don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['mailText'] = dataset['mailText'].str.lower()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tokenizing\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text) :\n",
    "    return word_tokenize(text)\n",
    "\n",
    "dataset['Tokenize Text'] = dataset['mailText'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "      <th>Tokenize Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText  \\\n",
       "0    ham  go until jurong point, crazy.. available only ...   \n",
       "1    ham                      ok lar... joking wif u oni...   \n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3    ham  u dun say so early hor... u c already then say...   \n",
       "4    ham  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Tokenize Text  \n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...  \n",
       "1           [ok, lar, ..., joking, wif, u, oni, ...]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [u, dun, say, so, early, hor, ..., u, c, alrea...  \n",
       "4  [nah, i, do, n't, think, he, goes, to, usf, ,,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Removing Noise:\n",
    "{removing irrelevant characters from the text}\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(tokens):\n",
    "    return [ token for token in tokens if not token.isdigit()]\n",
    "\n",
    "\n",
    "dataset['Tokenize Text'] = dataset['Tokenize Text'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuations(tokens):\n",
    "    return [ token for token in tokens if not token in string.punctuation ]\n",
    "\n",
    "dataset['Tokenize Text'] = dataset['Tokenize Text'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    return [ token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "dataset['Tokenize Text'] = dataset['Tokenize Text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Stemming\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "\n",
    "lang = \"english\"\n",
    "stemmer = SnowballStemmer(lang)\n",
    "def adding_Stemming(tokens):\n",
    "    return [ stemmer.stem(token) for token in tokens ]\n",
    "\n",
    "dataset['Tokenize Text'] = dataset['Tokenize Text'].apply(adding_Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "      <th>Tokenize Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, .., avail, bugi, n,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, ..., u, c, alreadi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText  \\\n",
       "0    ham  go until jurong point, crazy.. available only ...   \n",
       "1    ham                      ok lar... joking wif u oni...   \n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3    ham  u dun say so early hor... u c already then say...   \n",
       "4    ham  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Tokenize Text  \n",
       "0  [go, jurong, point, crazi, .., avail, bugi, n,...  \n",
       "1             [ok, lar, ..., joke, wif, u, oni, ...]  \n",
       "2  [free, entri, wkli, comp, win, fa, cup, final,...  \n",
       "3  [u, dun, say, earli, hor, ..., u, c, alreadi, ...  \n",
       "4  [nah, n't, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-Id3set (Term Frequency-Inverse Document Frequency):\n",
    "----\n",
    "\n",
    "TF-Id3set is a statistical measure that evaluates the importance of a word within a document relative to a collection of documents (corpus). It is commonly used for text-based information retrieval and text mining tasks like document classification, information retrieval, and document ranking. Here's how TF-Id3set works:\n",
    "\n",
    "Term Frequency (TF): This component measures how frequently a term (word) appears in a document. It is calculated as the number of times a term appears in a document divided by the total number of terms in the document.\n",
    "\n",
    "tf(t,d) = count of t in d / number of words in d\n",
    "\n",
    "Inverse Document Frequency (Id3set): This component measures the importance of a term across the entire corpus. It is calculated as the logarithm of the total number of documents in the corpus divided by the number of documents containing the term.\n",
    "\n",
    "id3set(t) = N/ d3set(t) = N/N(t)\n",
    "\n",
    "The TF-Id3set score of a term in a document is computed as TF multiplied by Id3set. It represents how unique and important a term is to a specific document relative to the entire corpus.\n",
    "\n",
    "TF-Id3set scores are typically used as feature vectors for documents in machine learning models. Each document is represented as a vector of TF-Id3set scores for its constituent terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "      <th>Tokenize Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, .., avail, bugi, n,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, ..., u, c, alreadi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText  \\\n",
       "0    ham  go until jurong point, crazy.. available only ...   \n",
       "1    ham                      ok lar... joking wif u oni...   \n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3    ham  u dun say so early hor... u c already then say...   \n",
       "4    ham  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Tokenize Text  \n",
       "0  [go, jurong, point, crazi, .., avail, bugi, n,...  \n",
       "1             [ok, lar, ..., joke, wif, u, oni, ...]  \n",
       "2  [free, entri, wkli, comp, win, fa, cup, final,...  \n",
       "3  [u, dun, say, earli, hor, ..., u, c, alreadi, ...  \n",
       "4  [nah, n't, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2set = dataset\n",
    "d2set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import Tfid3setVectorizer\n",
    "\n",
    "tfid3set_vectorizer = Tfid3setVectorizer()\n",
    "\n",
    "tfid3set_matrix = tfid3set_vectorizer.fit_transform([\" \".join(doc) for doc in d2set['Tokenize Text']])\n",
    "\n",
    "tfid3set_d3set = pd.DataFrame(tfid3set_matrix.toarray(), columns=tfid3set_vectorizer.get_feature_names_out())\n",
    "\n",
    "d2set = pd.concat([dataset, tfid3set_d3set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "      <th>Tokenize Text</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>03</th>\n",
       "      <th>...</th>\n",
       "      <th>ó_</th>\n",
       "      <th>û_</th>\n",
       "      <th>û_thank</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªv</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharri</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, .., avail, bugi, n,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, u, oni, ...]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, ..., u, c, alreadi, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goe, usf, live, around, though]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText  \\\n",
       "0    ham  go until jurong point, crazy.. available only ...   \n",
       "1    ham                      ok lar... joking wif u oni...   \n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3    ham  u dun say so early hor... u c already then say...   \n",
       "4    ham  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Tokenize Text   00  000  000pes   02  \\\n",
       "0  [go, jurong, point, crazi, .., avail, bugi, n,...  0.0  0.0     0.0  0.0   \n",
       "1             [ok, lar, ..., joke, wif, u, oni, ...]  0.0  0.0     0.0  0.0   \n",
       "2  [free, entri, wkli, comp, win, fa, cup, final,...  0.0  0.0     0.0  0.0   \n",
       "3  [u, dun, say, earli, hor, ..., u, c, alreadi, ...  0.0  0.0     0.0  0.0   \n",
       "4  [nah, n't, think, goe, usf, live, around, though]  0.0  0.0     0.0  0.0   \n",
       "\n",
       "   0207  02072069400   03  ...   ó_   û_  û_thank  ûªm  ûªt  ûªv   ûï  \\\n",
       "0   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "1   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "2   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "3   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "4   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   ûïharri   ûò  ûówel  \n",
       "0      0.0  0.0    0.0  \n",
       "1      0.0  0.0    0.0  \n",
       "2      0.0  0.0    0.0  \n",
       "3      0.0  0.0    0.0  \n",
       "4      0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 7073 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2set['Target'] = d2set['Target'].apply(lambda x : 1 if x == \"ham\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "      <th>Tokenize Text</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>03</th>\n",
       "      <th>...</th>\n",
       "      <th>ó_</th>\n",
       "      <th>û_</th>\n",
       "      <th>û_thank</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûªv</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûïharri</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, .., avail, bugi, n,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, u, oni, ...]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, ..., u, c, alreadi, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goe, usf, live, around, though]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                                           mailText  \\\n",
       "0       1  go until jurong point, crazy.. available only ...   \n",
       "1       1                      ok lar... joking wif u oni...   \n",
       "2       0  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3       1  u dun say so early hor... u c already then say...   \n",
       "4       1  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Tokenize Text   00  000  000pes   02  \\\n",
       "0  [go, jurong, point, crazi, .., avail, bugi, n,...  0.0  0.0     0.0  0.0   \n",
       "1             [ok, lar, ..., joke, wif, u, oni, ...]  0.0  0.0     0.0  0.0   \n",
       "2  [free, entri, wkli, comp, win, fa, cup, final,...  0.0  0.0     0.0  0.0   \n",
       "3  [u, dun, say, earli, hor, ..., u, c, alreadi, ...  0.0  0.0     0.0  0.0   \n",
       "4  [nah, n't, think, goe, usf, live, around, though]  0.0  0.0     0.0  0.0   \n",
       "\n",
       "   0207  02072069400   03  ...   ó_   û_  û_thank  ûªm  ûªt  ûªv   ûï  \\\n",
       "0   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "1   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "2   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "3   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "4   0.0          0.0  0.0  ...  0.0  0.0      0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   ûïharri   ûò  ûówel  \n",
       "0      0.0  0.0    0.0  \n",
       "1      0.0  0.0    0.0  \n",
       "2      0.0  0.0    0.0  \n",
       "3      0.0  0.0    0.0  \n",
       "4      0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 7073 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the date to Training and Testing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d2set.drop(['Target','mailText','Tokenize Text'],axis=1)\n",
    "Y = d2set['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Models from tf-id3set data\n",
    "---\n",
    "Naive Bayes\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier \n",
      "--------------------------------------------------------------\n",
      "Accuracy Score :  0.8708520179372198\n",
      "Calssification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.51      0.65       264\n",
      "           1       0.87      0.98      0.92       851\n",
      "\n",
      "    accuracy                           0.87      1115\n",
      "   macro avg       0.88      0.75      0.79      1115\n",
      "weighted avg       0.87      0.87      0.86      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "tf_naive = GaussianNB()\n",
    "tf_naive.fit(x_train,y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "pred_naive = tf_naive.predict(x_test)\n",
    "print(\"Naive Bayes Classifier \")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Accuracy Score : \",accuracy_score(pred_naive,y_test))\n",
    "print(\"Calssification Report : \\n\",classification_report(pred_naive,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      "--------------------------------------------------------------\n",
      "Accuracy Score :  0.9497757847533632\n",
      "Calssification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.96      0.78       102\n",
      "           1       1.00      0.95      0.97      1013\n",
      "\n",
      "    accuracy                           0.95      1115\n",
      "   macro avg       0.82      0.95      0.87      1115\n",
      "weighted avg       0.96      0.95      0.95      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tf_log = LogisticRegression()\n",
    "tf_log.fit(x_train,y_train)\n",
    "\n",
    "pred_log = tf_log.predict(x_test)\n",
    "print(\"Logistic Regression \")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Accuracy Score : \",accuracy_score(pred_log,y_test))\n",
    "print(\"Calssification Report : \\n\",classification_report(pred_log,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine \n",
      "--------------------------------------------------------------\n",
      "Accuracy Score :  0.9802690582959641\n",
      "Calssification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92       134\n",
      "           1       1.00      0.98      0.99       981\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.94      0.98      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "tf_svm = SVC(kernel='linear',C=1,random_state=42)\n",
    "tf_svm.fit(x_train,y_train)\n",
    "\n",
    "pred_svm = tf_svm.predict(x_test)\n",
    "print(\"Support Vector Machine \")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Accuracy Score : \",accuracy_score(pred_svm,y_test))\n",
    "print(\"Calssification Report : \\n\",classification_report(pred_svm,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "---\n",
    "\n",
    "It is an approach for representing words and documents. Word Embedding or Word Vector is a numeric vector input that represents a word in a lower-dimensional space. \n",
    "\n",
    "It allows words with similar meanings to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features.\n",
    "\n",
    "Features: Anything that relates words to one another. E.g.: Age, Sports, Fitness, Employed, etc. Each word vector has values corresponding to these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "      <th>Tokenize Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, .., avail, bugi, n,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, ..., u, c, alreadi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText  \\\n",
       "0    ham  go until jurong point, crazy.. available only ...   \n",
       "1    ham                      ok lar... joking wif u oni...   \n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3    ham  u dun say so early hor... u c already then say...   \n",
       "4    ham  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Tokenize Text  \n",
       "0  [go, jurong, point, crazi, .., avail, bugi, n,...  \n",
       "1             [ok, lar, ..., joke, wif, u, oni, ...]  \n",
       "2  [free, entri, wkli, comp, win, fa, cup, final,...  \n",
       "3  [u, dun, say, earli, hor, ..., u, c, alreadi, ...  \n",
       "4  [nah, n't, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3set = dataset\n",
    "d3set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Word2Vec:\n",
    "___\n",
    "\n",
    "In Word2Vec every word is assigned a vector. We start with either a random vector or one-hot vector.\n",
    "\n",
    "One-Hot vector: A representation where only one bit in a vector is 1.If there are 500 words in the corpus then the vector length will be 500.\n",
    "\n",
    "After assigning vectors to each word we take a window size and iterate through the entire corpus. While we do this there are two neural embedding methods which are used:\n",
    "\n",
    "    1.CBOW(Continuous Bag of Words)\n",
    "    2.Skip Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_text = d3set['Tokenize Text']\n",
    "model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(tokens, model, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            num_words += 1\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "\n",
    "    if num_words > 0:\n",
    "        feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector\n",
    "num_features = 100\n",
    "document_vectors = []\n",
    "for tokens in tokenized_text:\n",
    "    vector = document_vector(tokens, model, num_features)\n",
    "    document_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the date to Training and Testing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(document_vectors)\n",
    "Y = d3set['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier \n",
      "--------------------------------------------------------------\n",
      "Accuracy Score :  0.5004484304932736\n",
      "Calssification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.45      0.94      0.61       460\n",
      "        spam       0.83      0.19      0.31       655\n",
      "\n",
      "    accuracy                           0.50      1115\n",
      "   macro avg       0.64      0.57      0.46      1115\n",
      "weighted avg       0.67      0.50      0.43      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "w2v_naive = GaussianNB()\n",
    "w2v_naive.fit(x_train,y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "pred_naive = w2v_naive.predict(x_test)\n",
    "print(\"Naive Bayes Classifier \")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Accuracy Score : \",accuracy_score(pred_naive,y_test))\n",
    "print(\"Calssification Report : \\n\",classification_report(pred_naive,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      "--------------------------------------------------------------\n",
      "Accuracy Score :  0.8654708520179372\n",
      "Calssification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.87      0.93      1115\n",
      "        spam       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87      1115\n",
      "   macro avg       0.50      0.43      0.46      1115\n",
      "weighted avg       1.00      0.87      0.93      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "w2v_log = LogisticRegression()\n",
    "w2v_log.fit(x_train,y_train)\n",
    "\n",
    "pred_log = w2v_log.predict(x_test)\n",
    "print(\"Logistic Regression \")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Accuracy Score : \",accuracy_score(pred_log,y_test))\n",
    "print(\"Calssification Report : \\n\",classification_report(pred_log,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vectors Machines\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine \n",
      "--------------------------------------------------------------\n",
      "Accuracy Score :  0.8654708520179372\n",
      "Calssification Report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.87      0.93      1115\n",
      "        spam       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.87      1115\n",
      "   macro avg       0.50      0.43      0.46      1115\n",
      "weighted avg       1.00      0.87      0.93      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "w2v_svm = SVC(kernel='linear',C=1,random_state=42)\n",
    "w2v_svm.fit(x_train,y_train)\n",
    "\n",
    "pred_svm = w2v_svm.predict(x_test)\n",
    "print(\"Support Vector Machine \")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(\"Accuracy Score : \",accuracy_score(pred_svm,y_test))\n",
    "print(\"Calssification Report : \\n\",classification_report(pred_svm,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) GloVe (Global Vectors for Word Representation):\n",
    "___\n",
    "This is another method for creating word embeddings. In this method, we take the corpus and iterate through it and get the co-occurrence of each word with other words in the corpus.\n",
    "\n",
    "We get a co-occurrence matrix through this. The words which occur next to each other get a value of 1, if they are one word apart then 1/2, if two words apart then 1/3 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "      <th>Tokenize Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, .., avail, bugi, n,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, ..., u, c, alreadi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText  \\\n",
       "0    ham  go until jurong point, crazy.. available only ...   \n",
       "1    ham                      ok lar... joking wif u oni...   \n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3    ham  u dun say so early hor... u c already then say...   \n",
       "4    ham  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Tokenize Text  \n",
       "0  [go, jurong, point, crazi, .., avail, bugi, n,...  \n",
       "1             [ok, lar, ..., joke, wif, u, oni, ...]  \n",
       "2  [free, entri, wkli, comp, win, fa, cup, final,...  \n",
       "3  [u, dun, say, earli, hor, ..., u, c, alreadi, ...  \n",
       "4  [nah, n't, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "# Specify the path to your GloVe file\n",
    "glove_file = './glove.42B.300d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(token, embeddings_index):\n",
    "    return embeddings_index.get(token, np.zeros(300))  \n",
    "\n",
    "d3set['GloVe Embeddings'] = d3set['Tokenize Text'].apply(lambda tokens: [get_word_vector(token, glove_embeddings) for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>mailText</th>\n",
       "      <th>Tokenize Text</th>\n",
       "      <th>GloVe Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, .., avail, bugi, n,...</td>\n",
       "      <td>[[0.094418, 0.26803, -0.18872, -0.34682, 0.173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, u, oni, ...]</td>\n",
       "      <td>[[0.05973, 0.11751, -0.19544, -0.2859, 0.34065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "      <td>[[-0.61984, -0.31242, 0.39918, 0.48442, 0.1743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, ..., u, c, alreadi, ...</td>\n",
       "      <td>[[-0.078214, 0.95937, 0.12532, 0.52195, 0.0887...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goe, usf, live, around, though]</td>\n",
       "      <td>[[0.28848, 0.1572, 0.49064, -0.057261, -0.5658...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                           mailText  \\\n",
       "0    ham  go until jurong point, crazy.. available only ...   \n",
       "1    ham                      ok lar... joking wif u oni...   \n",
       "2   spam  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3    ham  u dun say so early hor... u c already then say...   \n",
       "4    ham  nah i don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       Tokenize Text  \\\n",
       "0  [go, jurong, point, crazi, .., avail, bugi, n,...   \n",
       "1             [ok, lar, ..., joke, wif, u, oni, ...]   \n",
       "2  [free, entri, wkli, comp, win, fa, cup, final,...   \n",
       "3  [u, dun, say, earli, hor, ..., u, c, alreadi, ...   \n",
       "4  [nah, n't, think, goe, usf, live, around, though]   \n",
       "\n",
       "                                    GloVe Embeddings  \n",
       "0  [[0.094418, 0.26803, -0.18872, -0.34682, 0.173...  \n",
       "1  [[0.05973, 0.11751, -0.19544, -0.2859, 0.34065...  \n",
       "2  [[-0.61984, -0.31242, 0.39918, 0.48442, 0.1743...  \n",
       "3  [[-0.078214, 0.95937, 0.12532, 0.52195, 0.0887...  \n",
       "4  [[0.28848, 0.1572, 0.49064, -0.057261, -0.5658...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target              object\n",
       "mailText            object\n",
       "Tokenize Text       object\n",
       "GloVe Embeddings    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "requriedData = d3set['GloVe Embeddings'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0.094418, 0.26803, -0.18872, -0.34682, 0.173...\n",
       "1       [[0.05973, 0.11751, -0.19544, -0.2859, 0.34065...\n",
       "2       [[-0.61984, -0.31242, 0.39918, 0.48442, 0.1743...\n",
       "3       [[-0.078214, 0.95937, 0.12532, 0.52195, 0.0887...\n",
       "4       [[0.28848, 0.1572, 0.49064, -0.057261, -0.5658...\n",
       "                              ...                        \n",
       "5567    [[0.10397, -0.20526, -0.29512, 0.016276, 0.007...\n",
       "5568    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "5569    [[-0.39909, -0.38653, 0.082255, 0.10243, -0.52...\n",
       "5570    [[-0.46733, 0.5856, 0.057228, -0.2123, -0.0979...\n",
       "5571    [[-0.2477, 0.14809, 0.047774, -0.50507, 0.1089...\n",
       "Name: GloVe Embeddings, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requriedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(embedding_list):\n",
    "    token_list = [f\"Token_{i}\" for i in range(len(embedding_list))]\n",
    "    data = []\n",
    "\n",
    "    for token, embedding in zip(token_list, embedding_list):\n",
    "        embedding_str = ' '.join(map(str, embedding))\n",
    "        data.append(f\"{token} {embedding_str}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Apply the function to the 'GloVe Embeddings' column\n",
    "impData = d3set['GloVe Embeddings'].apply(extract_embeddings)\n",
    "\n",
    "requriedData = pd.DataFrame(impData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GloVe Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Token_0 0.094418 0.26803 -0.18872 -0.34682 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Token_0 0.05973 0.11751 -0.19544 -0.2859 0.34...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Token_0 -0.61984 -0.31242 0.39918 0.48442 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Token_0 -0.078214 0.95937 0.12532 0.52195 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Token_0 0.28848 0.1572 0.49064 -0.057261 -0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>[Token_0 0.10397 -0.20526 -0.29512 0.016276 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>[Token_0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>[Token_0 -0.39909 -0.38653 0.082255 0.10243 -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>[Token_0 -0.46733 0.5856 0.057228 -0.2123 -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>[Token_0 -0.2477 0.14809 0.047774 -0.50507 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       GloVe Embeddings\n",
       "0     [Token_0 0.094418 0.26803 -0.18872 -0.34682 0....\n",
       "1     [Token_0 0.05973 0.11751 -0.19544 -0.2859 0.34...\n",
       "2     [Token_0 -0.61984 -0.31242 0.39918 0.48442 0.1...\n",
       "3     [Token_0 -0.078214 0.95937 0.12532 0.52195 0.0...\n",
       "4     [Token_0 0.28848 0.1572 0.49064 -0.057261 -0.5...\n",
       "...                                                 ...\n",
       "5567  [Token_0 0.10397 -0.20526 -0.29512 0.016276 0....\n",
       "5568  [Token_0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0...\n",
       "5569  [Token_0 -0.39909 -0.38653 0.082255 0.10243 -0...\n",
       "5570  [Token_0 -0.46733 0.5856 0.057228 -0.2123 -0.0...\n",
       "5571  [Token_0 -0.2477 0.14809 0.047774 -0.50507 0.1...\n",
       "\n",
       "[5572 rows x 1 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requriedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
